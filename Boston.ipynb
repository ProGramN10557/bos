# -*- coding: utf-8 -*-
"""
Created on Mon Sep 25 22:59:02 2017

@author: VishaalMM
"""

import pandas as pd
import numpy as np

df1=pd.read_csv('C:\\Users\\Radhika\\Documents\\Boston2.csv')
print(df1)

#Dimensions of Your Data
shape = df1.shape
print(shape)



description = df1.describe()
print(description)

types = df1.dtypes
print(types)

# co-relation heat-map
import matplotlib.pyplot as plt
import seaborn as sns
sns.set_style('whitegrid')
%matplotlib inline
sns.heatmap(df1.corr(), square=True,annot=True)

correlations = df1.corr(method='pearson')
print(correlations)

file='C:\\Users\\Radhika\\Documents\\output.csv'
correlations.to_csv(file)


# distribution of each feature (skewness check)
clr = ['blue', 'green', 'red']
fig, axs = plt.subplots(ncols=3,figsize=(15,3))

plt.figure(1)

for i, var in enumerate(['rm', 'lstat', 'ptratio']):
    plt.subplot(131 + i)
    sns.distplot(df1[var],  color = clr[i])
    plt.axvline(df1[var].mean(), color=clr[i], linestyle='solid', linewidth=2)
    plt.axvline(df1[var].median(), color=clr[i], linestyle='dashed', linewidth=2)

# assumption of linear regresion :  variable have normal distribution
# LSTAT is skewed to the right. U may have to do log transform to get normal distribution

# mark zero values as missing or NaN
df6=df1.replace(0, np.NaN)
df6['chas'].fillna(0, inplace=True)


import matplotlib.pylab as plt
df1.plot(kind='scatter', x='lstat', y='medv')


import matplotlib.pylab as plt
df1.plot(kind='scatter', x='zn', y='crim')
df6.plot(kind='scatter', x='zn', y='crim')
df6.plot(kind='bar', x='zn', y='crim')
df6.hist(column='zn', bins=50)
# trends
df1.plot(kind='scatter', x='nox', y='crim')
df1.plot(kind='scatter', x='medv', y='crim')
df1.plot(kind='scatter', x='lstat', y='crim')
df1.plot(kind='scatter', x='dis', y='crim')
df1.plot(kind='scatter', x='age', y='crim')
df1.plot(kind='scatter', x='nox', y='crim')


df7=df6.groupby(['zn'])['crim'].mean().reset_index()
#df7=df7.to_frame()
df7.rename(columns={'crim':'Avg Crime rate'}, inplace=True)
df7.plot(kind='bar', x='zn', y='Avg Crime rate')


df7=df6.groupby(['zn'])['tax'].mean().reset_index()
df7.rename(columns={'tax':'Avg tax rate'}, inplace=True)
df7.plot(kind='bar', x='zn', y='Avg tax rate')

df7=df6.groupby(['zn'])['ptratio'].mean().reset_index()
df7.rename(columns={'ptratio':'Avg ptratio'}, inplace=True)
df7.plot(kind='bar', x='zn', y='Avg ptratio')

df7=df6.groupby(['zn'])['ptratio'].median().reset_index()
df7.rename(columns={'ptratio':'median ptratio'}, inplace=True)

df7=df6.groupby(['zn'])['medv'].median().reset_index()
df7.rename(columns={'medv':'median value'}, inplace=True)
test = df7.sort_values(['median value'], ascending=[True])

df7=df6.groupby(['zn'])['rm'].max().reset_index()
df7.rename(columns={'rm':'max avg room per dwelling'}, inplace=True)
test = df7.sort_values(['max avg room per dwelling'], ascending=[False])
test.plot(kind='bar', x='zn', y='max avg room per dwelling')

test = df7.sort_values(['median value'], ascending=[True])

# linear regression one variable
X_nox=df1.iloc[:,5].values
y1=df1.iloc[:,1].values
reg_nox=LinearRegression()
reg_nox.fit(X_nox.reshape(-1,1),y1)
print(' The estimated intercept coefficient for NOX is %.2f ' %reg_nox.intercept_)


X_zn=df1.iloc[:,2].values
y1=df1.iloc[:,1].values
reg_zn=LinearRegression()
reg_zn.fit(X_zn.reshape(-1,1),y1)
print(' The estimated intercept coefficient for zn is %.2f ' %reg_zn.intercept_)


X_indus=df1.iloc[:,3].values
y1=df1.iloc[:,1].values
reg_indus=LinearRegression()
reg_indus.fit(X_indus.reshape(-1,1),y1)
print(' The estimated intercept coefficient for indus is %.2f ' %reg_indus.intercept_)

X_chas=df1.iloc[:,4].values
y1=df1.iloc[:,1].values
reg_chas=LinearRegression()
reg_chas.fit(X_chas.reshape(-1,1),y1)
print(' The estimated intercept coefficient for chas is %.2f ' %reg_chas.intercept_)

X_nox=df1.iloc[:,5].values
y1=df1.iloc[:,1].values
reg_nox=LinearRegression()
reg_nox.fit(X_nox.reshape(-1,1),y1)
print(' The estimated intercept coefficient for nox is %.2f ' %reg_nox.intercept_)

X_rm=df1.iloc[:,6].values
y1=df1.iloc[:,1].values
reg_rm=LinearRegression()
reg_rm.fit(X_rm.reshape(-1,1),y1)
print(' The estimated intercept coefficient for rm is %.2f ' %reg_rm.intercept_)

X_age=df1.iloc[:,7].values
y1=df1.iloc[:,1].values
reg_age=LinearRegression()
reg_age.fit(X_age.reshape(-1,1),y1)
print(' The estimated intercept coefficient for age is %.2f ' %reg_age.intercept_)

X_dis=df1.iloc[:,8].values
y1=df1.iloc[:,1].values
reg_dis=LinearRegression()
reg_dis.fit(X_dis.reshape(-1,1),y1)
print(' The estimated intercept coefficient for dis is %.2f ' %reg_dis.intercept_)

X_rad=df1.iloc[:,9].values
y1=df1.iloc[:,1].values
reg_rad=LinearRegression()
reg_rad.fit(X_rad.reshape(-1,1),y1)
print(' The estimated intercept coefficient for rad is %.2f ' %reg_rad.intercept_)

X_tax=df1.iloc[:,10].values
y1=df1.iloc[:,1].values
reg_tax=LinearRegression()
reg_tax.fit(X_tax.reshape(-1,1),y1)
print(' The estimated intercept coefficient for tax is %.2f ' %reg_tax.intercept_)

X_ptratio=df1.iloc[:,11].values
y1=df1.iloc[:,1].values
reg_ptratio=LinearRegression()
reg_ptratio.fit(X_ptratio.reshape(-1,1),y1)
print(' The estimated intercept coefficient for ptratio is %.2f ' %reg_ptratio.intercept_)

X_black=df1.iloc[:,12].values
y1=df1.iloc[:,1].values
reg_black=LinearRegression()
reg_black.fit(X_black.reshape(-1,1),y1)
print(' The estimated intercept coefficient for black is %.2f ' %reg_black.intercept_)

X_lstat=df1.iloc[:,13].values
y1=df1.iloc[:,1].values
reg_lstat=LinearRegression()
reg_lstat.fit(X_lstat.reshape(-1,1),y1)
print(' The estimated intercept coefficient for lstat is %.2f ' %reg_lstat.intercept_)

X_medv=df1.iloc[:,14].values
y1=df1.iloc[:,1].values
reg_medv=LinearRegression()
reg_medv.fit(X_medv.reshape(-1,1),y1)
print(' The estimated intercept coefficient for medv is %.2f ' %reg_medv.intercept_)








# Using seabron to create a linear fit
sns.lmplot('zn','crim',data = df1)
sns.lmplot('indus','crim',data = df1)
sns.lmplot('chas','crim',data = df1)
sns.lmplot('nox','crim',data = df1)
sns.lmplot('rm','crim',data = df1)
sns.lmplot('age','crim',data = df1)
sns.lmplot('dis','crim',data = df1)
sns.lmplot('rad','crim',data = df1)
sns.lmplot('tax','crim',data = df1)
sns.lmplot('ptratio','crim',data = df1)
sns.lmplot('black','crim',data = df1)
sns.lmplot('lstat','crim',data = df1)
sns.lmplot('medv','crim',data = df1)





# MULTIPLE LINEAR REGRESSION - backward elimination


X=df1.iloc[:,2:]  # selcts column 3 onwards ie from zn to medv
X=X.values
y=df1['crim'].values

# fit regressor to data set
 
import sklearn
from sklearn.linear_model import LinearRegression
lreg = LinearRegression()
lreg.fit(X,y)
print(' The estimated intercept coefficient is %.2f ' %lreg.intercept_)
print(' The number of coefficients used was %d ' % len(lreg.coef_))
# so now we have equation of line with 13 coeefieicnts (B1,B2,....B13) and intercept B0
from pandas import Series,DataFrame
import numpy
coeff_df = DataFrame(df1.columns)
coeff_df=coeff_df.iloc[2:]
coeff_df=coeff_df.reset_index()
coeff_df=coeff_df.drop('index',axis=1)
coeff_df.columns = ['Features']
coeff_df["Coefficient Estimate"] = pd.Series(lreg.coef_)

file='C:\\Users\\Radhika\\Documents\\output.csv'
coeff_df.to_csv(file)


# split into training and test
from sklearn.cross_validation import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2,random_state=0)

# fitting regressor to training set
from sklearn.linear_model import LinearRegression
regressor = LinearRegression()
regressor.fit(X_train, y_train)



# predciting test-set
y_pred=regressor.predict(X_test)

# backward elimination for statistical significance
import statsmodels.formula.api as sm
X=np.append(arr=np.ones((506,1)).astype(int), values = X, axis =1 )
# adding columns of 1 to X at begininng as statsmodels cannot read B0 in equatio B0+B1X1+B2X2
X_opt = X[:, [0,1,2,3,4,5,6,7,8,9,10,11,12,13]]
regressor_OLS = sm.OLS(endog=y, exog=X_opt).fit()
regressor_OLS.summary()

# Search for highest p- value and eliminate. Eliminate  x6 (age) which has p value = 0.936

X_opt = X[:, [0,1,2,3,4,5,7,8,9,10,11,12,13]]
regressor_OLS = sm.OLS(endog=y, exog=X_opt).fit()
regressor_OLS.summary()

# Search for highest p- value and eliminate. Eliminate  x3 (chas) which has p value = 0.528
# remember to compare Xopt with X becase ur ultimately removeing from x

X_opt = X[:, [0,1,2,4,5,7,8,9,10,11,12,13]]
regressor_OLS = sm.OLS(endog=y, exog=X_opt).fit()
regressor_OLS.summary()

# Search for highest p- value and eliminate. Eliminate  x7 (tax) which has p value = 0.5
# remember to compare Xopt with X becase ur ultimately removeing from x
# but tax is actually index 9 in X; so remove number 9

X_opt = X[:, [0,1,2,4,5,7,8,10,11,12,13]]
regressor_OLS = sm.OLS(endog=y, exog=X_opt).fit()
regressor_OLS.summary()


# Search for highest p- value and eliminate. Eliminate  x4 (RM) which has p value = 0.455
# remember to compare Xopt with X becase ur ultimately removeing from x
# but RMx is actually index 5 in X; so remove number 5

X_opt = X[:, [0,1,2,4,7,8,10,11,12,13]]
regressor_OLS = sm.OLS(endog=y, exog=X_opt).fit()
regressor_OLS.summary()

# Search for highest p- value and eliminate. Eliminate  x2 (INDUS) which has p value = 0.181
# remember to compare Xopt with X becase ur ultimately removeing from x
# here indiex is same as xopt; so remove 2


X_opt = X[:, [0,1,4,7,8,10,11,12,13]]
regressor_OLS = sm.OLS(endog=y, exog=X_opt).fit()
regressor_OLS.summary()

# Search for highest p- value and eliminate. Eliminate  x7 (LSTAT) which has p value = 0.112
# remember to compare Xopt with X becase ur ultimately removeing from x
# but LSTAT is actually index 12 in X. so remove 12


X_opt = X[:, [0,1,4,7,8,10,11,13]]
regressor_OLS = sm.OLS(endog=y, exog=X_opt).fit()
regressor_OLS.summary()

# Search for highest p- value and eliminate. Eliminate  x5 (PTRATO) which has p value = 0.071
# remember to compare Xopt with X becase ur ultimately removeing from x
# but PTRARIO is actually index 10 in X. so remove 10


X_opt = X[:, [0,1,4,7,8,11,13]]
regressor_OLS = sm.OLS(endog=y, exog=X_opt).fit()
regressor_OLS.summary()

# SO FINAL : ZN, NOX, DIS, RAD, B, MEDV

# polynomial regression between DIS and NOX

import matplotlib.pylab as plt
df1.plot(kind='scatter', x='dis', y='nox')

import seaborn as sns
sns.lmplot('dis','nox',data = df1)

X_dis=df1['dis']
X_dis_val=df1['dis'].values
# not a good shape, values are horizontal
x_dis_val1=df1.iloc[:,8:9].values # features ALWAYS MATRIX. now values are vertical
y_nox=df1['nox'].values # PREDICTOR ALWAYS ARRAY. values remain horizontal

# fit linear model for comparison sake
import sklearn
from sklearn.linear_model import LinearRegression
linregnox=LinearRegression()
linregnox.fit(x_dis_val1,y_nox)

# fit polynomial regression
from sklearn.preprocessing import PolynomialFeatures
poly_reg=PolynomialFeatures(degree=2)
X_poly=poly_reg.fit_transform(x_dis_val1)
poly_reg.fit(X_poly,y_nox)
linregnox_2=LinearRegression()
linregnox_2.fit(X_poly,y_nox)

# visualize linear regreeion plot
plt.scatter(x_dis_val1,y_nox,color='red')
plt.plot(x_dis_val1,linregnox.predict(x_dis_val1),color='blue')
plt.xlabel('dis')
plt.ylabel('nox')
plt.show()

# visulaise poly regression plot
plt.scatter(x_dis_val1,y_nox,color='red')
plt.plot(x_dis_val1,linregnox_2.predict(X_poly),color='blue')
plt.xlabel('dis')
plt.ylabel('nox')
plt.show()


















